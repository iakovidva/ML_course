{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWfr-9nXuLrV"
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "676hfhTnuLrV"
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JIneXDVQuLrW"
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBaMEnuluLrW"
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hJ9tgzduLrW",
    "outputId": "262baf97-a587-4333-c3af-ce586efa76f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_set_noclass.csv', <http.client.HTTPMessage at 0x7fbc50043590>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "USYVxSOquLrX"
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABI3uVFauLrX"
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure and accuracy of your models.\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDNmSLHIuLrX"
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three estimators/classifiers. Test both soft and hard voting and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VsgkJRUluLrX"
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cls1 = DecisionTreeClassifier(min_samples_leaf=100, min_impurity_decrease=0.001) # Classifier #1 \n",
    "cls2 = SGDClassifier() # Classifier #2 \n",
    "cls3 = ExtraTreeClassifier() #Classifier #3\n",
    "vcls = VotingClassifier([('dt1', cls1), ('dt2', cls2), ('dt3', cls3)], n_jobs=-1, voting=\"hard\") # Voting Classifier\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "accuracies = []\n",
    "fmeasures = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pred = vcls.fit(X_train,y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test,pred))\n",
    "    fmeasures.append(f1_score(y_test,pred))\n",
    "    \n",
    "avg_fmeasure = sum(fmeasures)/len(fmeasures) # The average f-measure\n",
    "avg_accuracy = sum(accuracies)/len(accuracies) # The average accuracy\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14DThtxouLrY",
    "outputId": "cbc50425-643b-47bc-f5b5-2641b51cf4c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('dt1',\n",
      "                              DecisionTreeClassifier(min_impurity_decrease=0.001,\n",
      "                                                     min_samples_leaf=100)),\n",
      "                             ('dt2', SGDClassifier()),\n",
      "                             ('dt3', ExtraTreeClassifier())],\n",
      "                 n_jobs=-1)\n",
      "F1-Score:0.8296609646646731 & Accuracy:0.7962330946698488\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(vcls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0EG8c89uLrZ"
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two estimators/classifiers. Try different classifiers for the combination of the initial classifiers. Report your results in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lGwaoEdGuLrZ"
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "cls1 = DecisionTreeClassifier(max_depth=5,splitter = \"best\",min_impurity_decrease=3e-6) # Classifier #1 \n",
    "cls2 = DecisionTreeClassifier(max_depth=7,splitter = \"random\",min_impurity_decrease=3e-6) # Classifier #2 \n",
    "scls = StackingClassifier([('dt1',cls1),('dt2',cls2)], n_jobs=-1) # Stacking Classifier\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "accuracies = []\n",
    "fmeasures = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pred = scls.fit(X_train,y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test,pred))\n",
    "    fmeasures.append(f1_score(y_test,pred))\n",
    "\n",
    "avg_fmeasure = sum(fmeasures)/len(fmeasures) # The average f-measure\n",
    "avg_accuracy = sum(accuracies)/len(accuracies) # The average accuracy\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WH0DOgp3uLra",
    "outputId": "ad48edb4-b087-4e40-ad47-2470a7cc4830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('dt1',\n",
      "                                DecisionTreeClassifier(max_depth=5,\n",
      "                                                       min_impurity_decrease=3e-06)),\n",
      "                               ('dt2',\n",
      "                                DecisionTreeClassifier(max_depth=7,\n",
      "                                                       min_impurity_decrease=3e-06,\n",
      "                                                       splitter='random'))],\n",
      "                   n_jobs=-1)\n",
      "F1-Score:0.7817459479093924 & Accuracy:0.7295005114217525\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za2cM47IuLra"
   },
   "source": [
    "### 1.3 Report the results ###  \n",
    "Report the results of your experiments in the following cell. How did you choose your initial classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGeuOocbuLra"
   },
   "source": [
    "1.1 Με hard-voting το μοντέλο είχε ελάχιστα καλύτερη απόδοση από ότι με soft-voting χρησιμοποιώντας μόνο DecisionTreeClassifier. Ωστόσο, μετά από δοκιμές με ένα DecisionTreeClassifier, ένα SGDClassifier και ένα ExtraTree έχω ακόμα καλύτερα αποτελέσματα πετυχαίνοντας F1-Score = 0.829 και Accuracy = 0.795, χωρίς όμως να έχω την δυνατότητα για soft-voting, γιατί ο SGDClassifier δεν έχει μέθοδο predict_proba. Γενικότερα φαίνεται η μέθοδος voting-classifier να αποδίδει καλύτερα με διαφορετικούς κατηγοριοποιητές, για αυτό και χρησιμοποιήθηκαν 3 διαφορετικοί, με τον extra-tree να κανει σχεδον την ιδια δουλεια με ενα απλο decision-tree απλά με πιο τυχαίες παραμέτρους. \n",
    "\n",
    "1.2\n",
    "Έγιναν διάφοροι έλεγχοι σε παραμέτρους 2 DecisionTreeClassifiers. Συγκεκριμένα μετρήθηκε η απόδοση για βάθος 5,10,15 στο πρώτο και 3,7,20 στο δεύτερο με όλους τους δυνατούς τους συνδιασμούς. Για τον τρόπο που προχωράει η επιλογή για τον επόμενο κόμβο(splitter best και random) καθώς και για τιμές min_impurity_decrease(1e-6,3e-6,1e-7) καθώς και όλοι οι συνδιασμοί τους. Έτσι δίνοντας στον πρώτο classifier max_depth=5,splitter = \"best\",min_impurity_decrease=3e-6 και στον δεύτερο max_depth=7,splitter = \"random\",min_impurity_decrease=3e-6 πετυχαίνει F1-Score = 0.785 και Accuracy = 0.733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDfCDDQWuLra"
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAeheQ8nuLra"
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one is produced with a different way from the ones discussed in the lecture for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1/accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cid_6kVluLrb"
   },
   "outputs": [],
   "source": [
    "##### BEGIN CODE HERE\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "\n",
    "ens1 = BaggingClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(),\n",
    "    n_estimators = 10,\n",
    "    max_features = 0.7,\n",
    "    n_jobs = -1) #Random Subspaces Method\n",
    "\n",
    "ens2 = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 15,\n",
    "    min_samples_split = 50,\n",
    "    bootstrap = True,\n",
    "    n_jobs = -1)  #Random Forest\n",
    "\n",
    "ens3 = AdaBoostClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(),\n",
    "    n_estimators = 50) #AdaBoost\n",
    "\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=200)\n",
    "\n",
    "names = ['bagging','random_forest','adaboost','simple_tree']\n",
    "classifiers = [ens1,ens2,ens3,tree]\n",
    "\n",
    "f_measures = dict()\n",
    "accuracies = dict()\n",
    "# Example f_measures = {'Simple Decision':0.8551, 'Ensemble with random ...': 0.92, ...}\n",
    "\n",
    "for name,classifier in zip(names,classifiers):\n",
    "    kf = KFold(n_splits=10)\n",
    "    my_accuracies = []\n",
    "    my_fmeasures = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pred = classifier.fit(X_train,y_train).predict(X_test)\n",
    "        my_accuracies.append(accuracy_score(y_test,pred))\n",
    "        my_fmeasures.append(f1_score(y_test,pred))\n",
    "\n",
    "    avg_fmeasure = sum(my_fmeasures)/len(my_fmeasures) # The average f-measure\n",
    "    avg_accuracy = sum(my_accuracies)/len(my_accuracies) # The average accuracy\n",
    "    f_measures[name] = avg_fmeasure\n",
    "    accuracies[name] = avg_accuracy\n",
    "    \n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSvYZbh_uLrb",
    "outputId": "6c5dadbe-947b-4a4d-b169-a9bf51aba319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_features=0.7,\n",
      "                  n_jobs=-1)\n",
      "RandomForestClassifier(max_depth=15, min_samples_split=50, n_jobs=-1)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
      "DecisionTreeClassifier(min_samples_leaf=200)\n",
      "Classifier:bagging -  F1:0.8049394778173184\n",
      "Classifier:random_forest -  F1:0.840539783357349\n",
      "Classifier:adaboost -  F1:0.7419835258308327\n",
      "Classifier:simple_tree -  F1:0.7668403294932741\n",
      "Classifier:bagging -  Accuracy:0.7740675076713263\n",
      "Classifier:random_forest -  Accuracy:0.7976622343448119\n",
      "Classifier:adaboost -  Accuracy:0.6968428230480737\n",
      "Classifier:simple_tree -  Accuracy:0.7111552449141947\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1:{}\".format(name,score))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  Accuracy:{}\".format(name,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TISSjHEwuLrb"
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf4-O-gbuLrb"
   },
   "source": [
    "1.Random Subspaces Method. Δουλεύει με όλα τα παραδείγματα του συνόλου αλλά όχι με όλα τα χαρακτηριστικά αυτού. Δημιουργία 10 DecisionTreeClassifiers κάθε ένα εκ των οποίων εκπαιδεύεται στο 70% των χαρακτηριστικών του συνόλου δεδομένων χρησιμοποιώντας όλους τους πόρους του συστήματος για παραλληλία.\n",
    "-Μέση ακρίβεια: 76.9%, Μέση τιμή f1Score: 80%\n",
    "\n",
    "2.RandomForestClassifier. Έχω 100 DecisionTreeClassifiers κάθε ένα από αυτά έχει μέγιστο βάθος 15, και ελάχιστο αριθμό παραδειγμάτων για σπάσιμο κόμβου 50, για αποφυγή overfitting. Χρησιμοποιούνται όλοι οι πυρήνες για την εκπαίδευση και την πρόβλεψη. Κάθε δέντρο εκπαιδεύεται σε κάποιο τυχαίο υποσύνολο του συνόλου δεδομένων, ίδιο μέγεθος με ολόκληρο το σύνολο, με επανατοποθέτηση. Το τελικό αποτέλεσμα προκύπτει από τον συνδιασμό όλων τον προβλέψεων των δέντρων.\n",
    "-Μέση ακρίβεια: 79.2%, Μέση τιμή f1Score: 83.3%\n",
    "\n",
    "3.AdaBoostClassifier. Δημιουργία 50 DecisionTreeClassifiers. Το πρώτο δέντρο κάνει fit και predict στα δεδομένα όπως ένα απλό δέντρο. Το δεύτερο δίνει μεγαλύτερο βάρος στα παραδείγματα στα οποία το πρώτο απέτυχε να βρει την σωστή κλάση. Το τρίτο δίνει μεγαλύτερο βάρος σε αυτά που απέτυχε το δεύτερο και ούτω καθεξής. Σε αυτή τη μέθοδο δεν υπάρχει παράλληλη επεξεργασία, καθώς για κάθε δέντρο χρησιμοποιεί το προηγούμενό του.\n",
    "-Μέση ακρίβεια: 69.7%, Μέση τιμή f1Score: 74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFoggFrYuLrb"
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnjMV2zcuLrc"
   },
   "source": [
    "Στην περίπτωση που αυξηθεί πολύ ο αριθμός των classifiers μπορεί να μειωθεί ο χρόνος εκτέλεσης χρησιμοποιώντας όλους τους πυρήνες του συστήματος, μοιράζοντας την δουλειά του κάθε classifier. Αυτό δεν μπορεί να επιτευχθεί σε έναν boosting classifier, καθώς η εκτέλεση του κάθε classifier γίνεται διαδοχικά. Ο κάθε classifier χρησιμοποιεί την έξοδο του προηγούμενού του για να εκτελεστεί."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzd4_C1huLrc"
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgKxB-huuLrc"
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve an accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfCU4AtNuLrc",
    "outputId": "eb70e48b-dde4-484e-9fba-4dbcfe0dcf8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "ens1 = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 15,\n",
    "    n_jobs = -1)\n",
    "ens2 = SGDClassifier()\n",
    "ens3 = ExtraTreeClassifier()\n",
    "ens4 = RandomForestClassifier(\n",
    "    n_estimators = 50)\n",
    "best_cls = StackingClassifier([('dt1', ens2), ('dt2', ens3), ('dt3', ens4), ('dt4', ens1)], n_jobs=-1)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "best_accuracy = []\n",
    "best_fmeasure = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pred = best_cls.fit(X_train,y_train).predict(X_test)\n",
    "    best_accuracy.append(accuracy_score(y_test,pred))\n",
    "    best_fmeasure.append(f1_score(y_test,pred))\n",
    "\n",
    "best_fmeasure = sum(best_fmeasure)/len(best_fmeasure)\n",
    "best_accuracy = sum(best_accuracy)/len(best_accuracy)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfsTCrvNuLrc",
    "outputId": "622cab5a-902a-4771-998a-d988364ae359",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('dt1', SGDClassifier()),\n",
      "                               ('dt2', ExtraTreeClassifier()),\n",
      "                               ('dt3', RandomForestClassifier(n_estimators=50)),\n",
      "                               ('dt4',\n",
      "                                RandomForestClassifier(max_depth=15,\n",
      "                                                       n_jobs=-1))],\n",
      "                   n_jobs=-1)\n",
      "F1-Score:0.8673898951147857 & Accuracy:0.8403216274576655\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(best_cls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(best_fmeasure,best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtogTNu-uLrc"
   },
   "source": [
    "###### **3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXxLSf4wuLrd"
   },
   "source": [
    "Έγιναν διάφοροι συνδιασμοί προηγούμενων classifiers για την υλοποιήση του τελικού. Συγκεκριμένα υπολόγισα την ακρίβεια δοκιμάζοντας τόσο με VotingClassifier όσο και με StackingClassifier για διάφορα σύνολα κατηγοριοποιητών. Για παράδειγμα με 3 RandomForest και StackingClassifier με ακρίβεια περίπου 81.5%, με VotingClassifier δεν είχε τόσο καλή απόδοση ενώ με hard voting ήταν αρκετά χειρότερη. Δοκίμασα να βάλω δύο RandomForest μαζί με έναν BaggingClassifier σε έναν VottingClassifier πετυχαίνοντας ακρίβεια γύρω στο 78%. Δοκιμάστηκαν και άλλες παραλλαγές με classifiers όπως ο ExtraTreeClassifier, SGDClassifier μαζί με κάποιο RandomForest και όλα αυτά μέσα σε έναν Voting ή Stacking Classifier.\n",
    "\n",
    "Καλύτερα αποτελέσματα έδινε ένας StackingClassifier με δύο RandomForestClassifiers, έναν SGDClassifier και έναν ExtraTreeClassifier μέσα σε αυτόν. \n",
    "\n",
    "Έπειτα δοκιμάστηκαν κάποιες μεταθέσεις στη σειρά που είχαν οι παραπάνω classifiers μέσα στον Stacking παίρνοντας ελαφρώς διαφορετικά αποτελέσματα. Ο καλύτερος κατηγοριοποιητής που κατέληξα πετυχαίνει F1-Score:0.8688 και Accuracy:0.8436. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqHvyBwpuLrd"
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptylg15NuLrd",
    "outputId": "ddab0735-69a3-4f4e-f4e0-00c15c1a1b24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "# Τελικός κατηγοριοποιητής είναι ο ίδιος που απέδιδε καλύτερα και στο παραπάνω ερώτημα.\n",
    "ens1 = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 15,\n",
    "    n_jobs = -1,\n",
    "    random_state=RANDOM_STATE)\n",
    "ens2 = SGDClassifier(random_state=RANDOM_STATE)\n",
    "ens3 = ExtraTreeClassifier(random_state=RANDOM_STATE)\n",
    "ens4 = RandomForestClassifier(\n",
    "    n_estimators = 50,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "cls = StackingClassifier([('dt1', ens2), ('dt2', ens3), ('dt3', ens4), ('dt4', ens1)], n_jobs=-1)\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\").sample(frac=1).reset_index(drop=True)\n",
    "predictions = cls.fit(X,y).predict(test_set)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhZPJuO6uLre",
    "outputId": "8b58d41e-d01a-4d27-8878-da07ee5f44cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier(estimators=[('dt1', SGDClassifier(random_state=42)),\n",
      "                               ('dt2', ExtraTreeClassifier(random_state=42)),\n",
      "                               ('dt3',\n",
      "                                RandomForestClassifier(n_estimators=50,\n",
      "                                                       random_state=42)),\n",
      "                               ('dt4',\n",
      "                                RandomForestClassifier(max_depth=15, n_jobs=-1,\n",
      "                                                       random_state=42))],\n",
      "                   n_jobs=-1)\n",
      "[0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(cls)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EnsembleMethods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
